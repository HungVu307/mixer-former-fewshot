{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zMUZOkp-DYqrJG7BOgClWTVR5jjE90_E","timestamp":1687658991012}],"collapsed_sections":["kOTfl-yC74vh"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgOmKNAJl5Eh","executionInfo":{"status":"ok","timestamp":1687677505368,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"2186da0e-9fee-4b6b-a836-19ee684a790c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jun 25 07:18:24 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cu2kO7dI55Cm","executionInfo":{"status":"ok","timestamp":1687677533602,"user_tz":-420,"elapsed":28236,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"ee5d0153-dc06-41ae-9f2a-c0a47fb96732"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1KegY9MkG7rSPR0m2aJ98a9ELxxpGpfC6/Bearing Faults Project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Python/Bearing Faults Project'"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from random import randint\n","import utils\n","import time\n","#import utils_lenet5\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import clear_output\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","bs = 1"],"metadata":{"id":"QNg0bgqd6BvI","executionInfo":{"status":"ok","timestamp":1687677541349,"user_tz":-420,"elapsed":7750,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"118ea5e2-41ea-4591-b926-05fe3c60e774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["#preprocessing"],"metadata":{"id":"kOTfl-yC74vh"}},{"cell_type":"code","source":["import re\n","def Get_Label(file_name):\n","    fault = re.search(r'([A-Za-z]+)', file_name).group()\n","    return fault\n","faults_map = {\n","    'N': 0,\n","    'I': 1,\n","    'O': 2,\n","    'B': 3,\n","    'IO': 4,\n","    'IB': 5,\n","    'OB': 6,\n","}"],"metadata":{"id":"JrBGJbnO6Exg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import scipy.io\n","import scipy.io as sio\n","import torch\n","\n","x_train = torch.rand(21000, 2048, 2)\n","y_train = torch.zeros(21000)\n","segment_length = 2048\n","overlap = 0.75\n","folder_path = \"/content/drive/MyDrive/Python/Bearing Faults Project/HUST bearing dataset\"\n","file_list = os.listdir(folder_path)\n","mat_files = [f for f in file_list if f.endswith('.mat')]\n","idx = 0\n","count_file = 0\n","for file_name in mat_files:\n","    file_path = os.path.join(folder_path, file_name)\n","    data = scipy.io.loadmat(file_path)\n","    bearing_data = data['data']\n","    bearing_data = torch.from_numpy(bearing_data)\n","    #y_train[idx] = faults_map[Get_Label(file_name)]\n","    segments = []\n","    total_length = bearing_data.shape[0]\n","    stride = int(segment_length * (1 - overlap))\n","    check_label = faults_map[Get_Label(file_name)]\n","    if check_label == 3:\n","        num_segments = 250\n","    elif check_label == 5:\n","        num_segments = 250\n","    else:\n","        num_segments = 200\n","    for i in range(num_segments):\n","        y_train[idx] = faults_map[Get_Label(file_name)]\n","        start = i * stride\n","        end = start + segment_length\n","        segment = bearing_data[start:end]\n","        segment = torch.cat((segment, segment), dim=1)\n","        x_train[idx] = segment\n","        idx += 1\n"],"metadata":{"id":"_1NYSpGr6GuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_data, test_data, train_label, test_label = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n","print(train_data.shape)\n","print(train_label.shape)\n","print(test_data.shape)\n","print(test_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPWKmZDJ6Kxf","executionInfo":{"status":"ok","timestamp":1687677681913,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"d990b68b-e914-455e-eafc-5a721ebb739c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16800, 2048, 2])\n","torch.Size([16800])\n","torch.Size([4200, 2048, 2])\n","torch.Size([4200])\n"]}]},{"cell_type":"code","source":["train_data = train_data.reshape(16800, 4096)\n","test_data = test_data.reshape(4200, 4096)\n","#\n","train_data_bf_spec = train_data\n","test_data_bf_spec = test_data"],"metadata":{"id":"1z46l-WQ6Q0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Transform to spectrogram\n","import librosa\n","import numpy as np\n","import cv2\n","\n","#change to spectrogram TRAIN_DATA\n","spectrograms = []\n","for i in range(train_data.shape[0]):\n","    signal = train_data[i, :]\n","    signal = np.array(signal)\n","    spectrogram = librosa.stft(signal, n_fft=512, hop_length=512)\n","    spectrogram = np.abs(spectrogram)**2\n","    log_spectrogram = librosa.power_to_db(spectrogram)\n","    log_spectrogram = cv2.resize(log_spectrogram, (64, 64))\n","    spectrograms.append(log_spectrogram)\n","train_data = np.stack(spectrograms)\n","\n","\n","#change to spectrogram TEST_DATA\n","\n","spectrograms_testdata = []\n","for i in range(test_data.shape[0]):\n","    signal = test_data[i, :]\n","    signal = np.array(signal)\n","    spectrogram = librosa.stft(signal, n_fft=512, hop_length=512)\n","    spectrogram = np.abs(spectrogram)**2\n","    log_spectrogram = librosa.power_to_db(spectrogram)\n","    log_spectrogram = cv2.resize(log_spectrogram, (64, 64))\n","    spectrograms_testdata.append(log_spectrogram)\n","test_data = np.stack(spectrograms_testdata)\n","\n","#to tensor\n","train_data = train_data.astype(np.float32)\n","test_data = test_data.astype(np.float32)\n","train_data = torch.from_numpy(train_data)\n","test_data = torch.from_numpy(test_data)\n","\n","train_data = train_data.unsqueeze(dim = 1)\n","test_data = test_data.unsqueeze(dim = 1)\n","\n","print(train_data.shape)\n","print(test_data.shape)\n","print(train_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09G2o_Ml6Y-5","executionInfo":{"status":"ok","timestamp":1687677713750,"user_tz":-420,"elapsed":31840,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"d7cadde3-2c01-43c3-afcf-4a42ac5b93a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16800, 1, 64, 64])\n","torch.Size([4200, 1, 64, 64])\n","torch.Size([16800])\n"]}]},{"cell_type":"markdown","source":["#Split the set of traindata"],"metadata":{"id":"hfZ0kBpQmRhF"}},{"cell_type":"code","source":["#-----Test with limited data-------#\n","train_data_10, test_data_10, train_label_10, test_label_10 = train_test_split(train_data, train_label, test_size=0.9, random_state=42)\n","\n","print(train_data_10.shape)\n","print(train_label_10.shape)\n","print(test_data.shape)\n","print(test_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJXFUMfxmWwe","executionInfo":{"status":"ok","timestamp":1687677720522,"user_tz":-420,"elapsed":482,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"bac78aa2-edbc-4b24-e4b4-7997dbf646ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1680, 1, 64, 64])\n","torch.Size([1680])\n","torch.Size([4200, 1, 64, 64])\n","torch.Size([4200])\n"]}]},{"cell_type":"markdown","source":["#Covmnet\n"],"metadata":{"id":"kzClDHF07752"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, train_data, train_label, episode_num=1000, way_num=7, shot_num=1, query_num=1):\n","        self.train_data = train_data\n","        self.train_label = train_label\n","        self.episode_num = episode_num\n","        self.way_num = way_num\n","        self.shot_num = shot_num\n","        self.query_num = query_num\n","\n","    def __len__(self):\n","        return self.episode_num\n","\n","    def __getitem__(self, index):\n","        query_images = []\n","        query_targets = []\n","        support_images = []\n","        support_targets = []\n","\n","        label_indices = torch.randperm(len(self.train_label)).cuda()\n","        train_label_gpu = self.train_label.cuda()\n","        train_data_gpu = self.train_data.cuda()\n","\n","        for label_num in range(self.way_num):\n","            support_idxs = torch.nonzero(train_label_gpu[label_indices] == label_num, as_tuple=False).flatten()\n","            support_idxs = support_idxs[:self.shot_num]\n","            support_data = train_data_gpu[label_indices][support_idxs]\n","\n","            query_idxs = torch.nonzero(train_label_gpu[label_indices] == label_num, as_tuple=False).flatten()\n","            query_idxs = query_idxs[~torch.isin(query_idxs, support_idxs)][:self.query_num]\n","            query_data = train_data_gpu[label_indices][query_idxs]\n","            query_data_targets = train_label_gpu[label_indices][query_idxs]\n","\n","            query_images.append(query_data)\n","            query_targets.append(query_data_targets)\n","            support_images.append(support_data)\n","            support_targets.append(torch.full((self.shot_num,), label_num).cuda())\n","\n","        query_images = torch.cat(query_images, dim=0)\n","        query_targets = torch.cat(query_targets, dim=0)\n","        support_images = torch.cat(support_images, dim=0)\n","        support_targets = torch.cat(support_targets, dim=0)\n","\n","        return query_images, query_targets, support_images, support_targets\n","\n","\n","\n","'''\n","  way_num = num_classes, shot_num = number samples per class\n","\n","'''\n","train_dataset = CustomDataset(train_data_10, train_label_10, episode_num=1000, way_num=7, shot_num=5, query_num=1)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n","test_dataset = CustomDataset(test_data, test_label, episode_num=100, way_num=7, shot_num=5, query_num=1)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"],"metadata":{"id":"ebzdjNfFTYoy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Check Custom dataset"],"metadata":{"id":"9YkL6tb5srLB"}},{"cell_type":"markdown","source":["###Function"],"metadata":{"id":"BBsItLlVu4CS"}},{"cell_type":"code","source":["def Convert_For_5shots(support_images, support_targets):\n","    support_targets = support_targets.cpu()\n","    labels = torch.unique(support_targets)\n","    new_support_images = []\n","    for label in labels:\n","        label_images = support_images[:, support_targets[0] == label]\n","        padded_label_images = torch.zeros((7, 1, 64, 64), dtype=label_images.dtype)\n","        padded_label_images[:label_images.shape[1]] = label_images.squeeze(0)\n","        new_support_images.append(padded_label_images.to(device))\n","    return new_support_images\n","\n","def Cal_Accuracy(loader, net):\n","    true_label = 0\n","    num_batches = 0\n","    for query_images, query_targets, support_images, support_targets in loader:\n","        q = query_images.permute(1, 0, 2, 3, 4).to(device)\n","        s = Convert_For_5shots(support_images, support_targets)\n","        targets = query_targets.to(device)\n","        targets = targets.permute(1,0)\n","        for i in range(len(q)):\n","            scores = net(q[i], s).float()\n","            target = targets[i].long()\n","            true_label += 1 if torch.argmax(scores) == target else 0\n","            num_batches += 1\n","    return true_label/num_batches\n"],"metadata":{"id":"3yY8Qhh9g1M9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"z9mcSAmfxFuy"}},{"cell_type":"markdown","source":["####Preparation"],"metadata":{"id":"yRvce5pHwoDu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","#-----------------------Convmixer-----------------------------------------------#\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(x) + x\n","\n","def ConvMixer(inplane = 1, dim = 64, depth=1, kernel_size=64, patch_size=4):\n","    return nn.Sequential(\n","        nn.Conv2d(inplane , dim, kernel_size=patch_size, stride=patch_size),\n","        nn.GELU(),\n","        nn.BatchNorm2d(dim),\n","        *[nn.Sequential(\n","            Residual(nn.Sequential(\n","                nn.Conv2d(dim, dim, (1, kernel_size), groups=dim, padding=\"same\"),\n","                nn.GELU(),\n","                nn.BatchNorm2d(dim),\n","                nn.Conv2d(dim, dim, (kernel_size, 1), groups=dim, padding=\"same\"),\n","                nn.GELU(),\n","                nn.BatchNorm2d(dim)\n","            )),\n","            nn.Conv2d(dim, dim, kernel_size=1),\n","            nn.GELU(),\n","            nn.BatchNorm2d(dim)\n","        ) for i in range(depth)]\n","    )\n","\n","#--------Multi-head attention Module-----------------------------------------------------------------------#\n","class MHSA(nn.Module):\n","    def __init__(self, n_dims = 64, width=16, height=16, heads=4):\n","        super(MHSA, self).__init__()\n","        self.heads = heads\n","\n","        self.query = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n","        self.key = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n","        self.value = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n","\n","        self.rel_h = nn.Parameter(torch.randn([1, heads, n_dims // heads, 1, height]), requires_grad=True)\n","        self.rel_w = nn.Parameter(torch.randn([1, heads, n_dims // heads, width, 1]), requires_grad=True)\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.norm = nn.LayerNorm([n_dims, width, height])\n","\n","    def forward(self, x):\n","        n_batch, C, width, height = x.size()\n","        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n","        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n","        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n","\n","        content_content = torch.matmul(q.permute(0, 1, 3, 2), k)\n","\n","        content_position = (self.rel_h + self.rel_w).view(1, self.heads, C // self.heads, -1).permute(0, 1, 3, 2)\n","        content_position = torch.matmul(content_position, q)\n","\n","        energy = content_content + content_position\n","        attention = self.softmax(energy)\n","\n","        out = torch.matmul(v, attention.permute(0, 1, 3, 2))\n","        out = out.view(n_batch, C, width, height)\n","        out = self.norm(out) + x\n","        return out\n","'''\n","test = torch.rand(1, 1, 64, 64)\n","mixer = ConvMixer()\n","model = MHSA()\n","output = model(mixer(test))\n","output.shape\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"WYqZH5Pq8GyV","executionInfo":{"status":"ok","timestamp":1687677736409,"user_tz":-420,"elapsed":658,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"2f16658d-3316-45ef-99de-d537e9655613"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntest = torch.rand(1, 1, 64, 64)\\nmixer = ConvMixer()\\nmodel = MHSA()\\noutput = model(mixer(test))\\noutput.shape\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["####Proposed"],"metadata":{"id":"ZagAZoExwrMm"}},{"cell_type":"code","source":["import functools\n","\n","class CovarianceNet_64(nn.Module):\n","    def __init__(self, norm_layer=nn.BatchNorm2d, num_classes=7):\n","        super(CovarianceNet_64, self).__init__()\n","\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        self.mixer = ConvMixer()\n","        self.MHSA = MHSA()\n","\n","        self.covariance = CovaBlock()\n","\n","        self.classifier = nn.Sequential(\n","            nn.LeakyReLU(0.2, True),\n","            nn.Dropout(),\n","            nn.Conv1d(1, 1, kernel_size=256, stride=256, bias=use_bias),\n","        )\n","\n","\n","    def forward(self, input1, input2):\n","        #q = self.features(input1)\n","        q = self.MHSA(self.mixer(input1))\n","        S = []\n","        for i in range(len(input2)):\n","            features = self.mixer(input2[i])\n","            S.append(self.MHSA(features))\n","        x = self.covariance(q, S)\n","        x = self.classifier(x.view(x.size(0), 1, -1))\n","        x = x.squeeze(1)\n","\n","\n","        return x\n","\n","\n","class CovaBlock(nn.Module):\n","    def __init__(self):\n","        super(CovaBlock, self).__init__()\n","\n","    def cal_covariance(self, input):\n","        CovaMatrix_list = []\n","        for i in range(len(input)):\n","            support_set_sam = input[i]\n","            B, C, h, w = support_set_sam.size()\n","\n","            support_set_sam = support_set_sam.permute(1, 0, 2, 3)\n","            support_set_sam = support_set_sam.contiguous().view(C, -1)\n","            mean_support = torch.mean(support_set_sam, 1, True)\n","            support_set_sam = support_set_sam - mean_support\n","\n","            covariance_matrix = support_set_sam @ torch.transpose(support_set_sam, 0, 1)\n","            covariance_matrix = torch.div(covariance_matrix, h * w * B - 1)\n","            CovaMatrix_list.append(covariance_matrix)\n","        return CovaMatrix_list\n","\n","    def cal_similarity(self, input, CovaMatrix_list):\n","        B, C, h, w = input.size()\n","        Cova_Sim = []\n","\n","        for i in range(B):\n","            query_sam = input[i]\n","            query_sam = query_sam.view(C, -1)\n","            query_sam_norm = torch.norm(query_sam, 2, 1, True)\n","            query_sam = query_sam / query_sam_norm\n","\n","            if torch.cuda.is_available():\n","                mea_sim = torch.zeros(1, len(CovaMatrix_list) * h * w).cuda()\n","            else:\n","                mea_sim = torch.zeros(1, len(CovaMatrix_list) * h * w)\n","            for j in range(len(CovaMatrix_list)):\n","                temp_dis = torch.transpose(query_sam, 0, 1) @ CovaMatrix_list[j] @ query_sam\n","                mea_sim[0, j * h * w:(j + 1) * h * w] = temp_dis.diag()\n","\n","            Cova_Sim.append(mea_sim.view(1, -1))\n","\n","        Cova_Sim = torch.cat(Cova_Sim, 0)\n","\n","        return Cova_Sim\n","\n","    def forward(self, x1, x2):\n","        CovaMatrix_list = self.cal_covariance(x2)\n","        Cova_Sim = self.cal_similarity(x1, CovaMatrix_list)\n","\n","        return Cova_Sim\n"],"metadata":{"id":"NsMkDUSt_haJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["/'''\n","model = CovarianceNet_64().cuda()\n","x1 = torch.rand(7,5,1,64,64).cuda()\n","x2 = torch.rand(1,1,64,64).cuda()\n","model(x2,x1).shape\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJ5NmylZQD-d","executionInfo":{"status":"ok","timestamp":1687662939183,"user_tz":-420,"elapsed":4,"user":{"displayName":"3 ComputerVision","userId":"09043009105184641819"}},"outputId":"5c24f1db-2770-4848-cc33-293b38431305"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 7])"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["#Training\n"],"metadata":{"id":"5aEcu07Y-SGV"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(device)"],"metadata":{"id":"7xUp4fRXg23m","executionInfo":{"status":"ok","timestamp":1687677746749,"user_tz":-420,"elapsed":515,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f102e9b-dbf1-49a7-d85b-1a6e755ca817"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["#device= torch.device(\"cuda\")\n","net = CovarianceNet_64()\n","net.to(device)\n","lr = 5e-3\n","criterion = nn.CrossEntropyLoss()\n"],"metadata":{"id":"AMWsq_17DIFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(net.parameters(), lr=lr)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","criterion.to(device)\n","full_loss = []\n","pred_acc = 0\n","\n","for epoch in range(1, 30):\n","    running_loss = 0\n","    num_batches = 0\n","    optimizer.zero_grad()\n","\n","    for query_images, query_targets, support_images, support_targets in train_dataloader:\n","        '''\n","        input: query_images (size: 16 (batchsize) x 7(catagories) x 1(chanel) x 64(H) x 64(W))\n","              support_images (size: 16 (batchsize) x 7(catagories) x 1(chanel) x 64(H) x 64(W)) (One shot)\n","\n","              each query_images will compare the covariance with each sample in the support set\n","        '''\n","        q = query_images.permute(1, 0, 2, 3, 4).to(device)\n","        s = Convert_For_5shots(support_images, support_targets)\n","        targets = query_targets.to(device)\n","        targets = targets.permute(1,0)\n","        for i in range(len(q)):\n","            scores = net(q[i], s).float()\n","            target = targets[i].long()\n","            loss = criterion(scores, target)\n","            loss.backward()\n","            running_loss += loss.detach().item()\n","            num_batches += 1\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    scheduler.step()\n","\n","    with torch.no_grad():\n","        #-----On training set-----------#\n","        total_loss = running_loss / num_batches\n","        full_loss.append(total_loss)\n","        print('epoch =', epoch, '\\t lr =', optimizer.param_groups[0]['lr'], '\\t loss =', total_loss)\n","        print('------------Testing on the test set-------------')\n","        acc = Cal_Accuracy(test_dataloader, net)\n","        print(f'Accuracy on the test set: {acc:.4f}')\n","        if acc > pred_acc:\n","            pred_acc = acc\n","            model_name = f'Covamnet_5shot_{acc:.4f}.pth'\n","            torch.save(net, '/content/drive/MyDrive/Python/Bearing Faults Project/' + model_name)\n","            print(f'=> Save the best model with accuracy is: {acc:.4f}')\n","        print(\"-------------------------------------------------------------------------------------------\")\n","\n","    torch.cuda.empty_cache()\n"],"metadata":{"id":"uauOPs6MDQde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'best accuracy: {pred_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SW1VXHLf0HI1","executionInfo":{"status":"ok","timestamp":1687684361540,"user_tz":-420,"elapsed":543,"user":{"displayName":"Hùng Vũ","userId":"17304875601276089938"}},"outputId":"23d7ad03-11d1-4eda-f8f0-a62e9ef826cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best accuracy: 0.9800\n"]}]},{"cell_type":"code","source":["plt.plot(full_loss)"],"metadata":{"id":"rge_Io3Fu7i8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Validation"],"metadata":{"id":"Z-mE4Y9bMSHk"}}]}